ngram-count(1)                                     ngram-count(1)



NNAAMMEE
       ngram-count - count N-grams and estimate language models

SSYYNNOOPPSSIISS
       nnggrraamm--ccoouunntt [ --hheellpp ] _o_p_t_i_o_n ...

DDEESSCCRRIIPPTTIIOONN
       nnggrraamm--ccoouunntt  generates  and manipulates N-gram counts, and
       estimates N-gram language models from them.   The  program
       first builds an internal N-gram count set, either by read-
       ing counts from a file, or by scanning text  input.   Fol-
       lowing  that, the resulting counts can be output back to a
       file or used for building an N-gram language model in ARPA
       nnggrraamm--ffoorrmmaatt(5).   Each  of  these actions is triggered by
       corresponding options, as described below.

OOPPTTIIOONNSS
       Each filename argument can be an ASCII  file,  or  a  com-
       pressed file (name ending in .Z or .gz), or ``-'' to indi-
       cate stdin/stdout.

       --hheellpp  Print option summary.

       --vveerrssiioonn
              Print version information.

       --oorrddeerr _n
              Set the maximal order (length) of N-grams to count.
              This also determines the order of the estimated LM,
              if any.  The default order is 3.

       --vvooccaabb _f_i_l_e
              Read a vocabulary from file.  Subsequently, out-of-
              vocabulary   words  in  both  counts  or  text  are
              replaced with  the  unknown-word  token.   If  this
              option is not specified all words found are implic-
              itly added to the vocabulary.

       --vvooccaabb--aalliiaasseess _f_i_l_e
              Reads vocabulary alias definitions from _f_i_l_e,  con-
              sisting of lines of the form
                   _a_l_i_a_s _w_o_r_d
              This  causes all tokens _a_l_i_a_s to be mapped to _w_o_r_d.

       --wwrriittee--vvooccaabb _f_i_l_e
              Write the vocabulary built in the counting  process
              to _f_i_l_e.

       --wwrriittee--vvooccaabb--iinnddeexx _f_i_l_e
              Write the internal word-to-integer mapping to _f_i_l_e.

       --ttaaggggeedd
              Interpret  text  and  N-grams  as   consisting   of
              word/tag pairs.

       --ttoolloowweerr
              Map all vocabulary to lowercase.

       --mmeemmuussee
              Print memory usage statistics.

   CCoouunnttiinngg OOppttiioonnss
       --tteexxtt _t_e_x_t_f_i_l_e
              Generate  N-gram  counts  from text file.  _t_e_x_t_f_i_l_e
              should  contain  one  sentence   unit   per   line.
              Begin/end  sentence tokens are added if not already
              present.  Empty lines are ignored.

       --nnoo--ssooss
              Disable the automatic  insertion  of  start-of-sen-
              tence tokens in N-gram counting.

       --nnoo--eeooss
              Disable  the automatic insertion of end-of-sentence
              tokens in N-gram counting.

       --rreeaadd _c_o_u_n_t_s_f_i_l_e
              Read N-gram counts from a file.  Ascii count  files
              contain  one  N-gram of words per line, followed by
              an integer  count,  all  separated  by  whitespace.
              Repeated  counts  for  the  same  N-gram are added.
              Thus several count files can  be  merged  by  using
              ccaatt(1)  and feeding the result to nnggrraamm--ccoouunntt --rreeaadd
              -- (but see nnggrraamm--mmeerrggee(1) for merging  counts  that
              exceed  available  memory).   Counts  collected  by
              --tteexxtt and --rreeaadd are additive as well.  Binary count
              files (see below) are also recognized.

       --rreeaadd--ggooooggllee _d_i_r
              Read  N-grams  counts  from  an  indexed  directory
              structure rooted in ddiirr, in a format  developed  by
              Google to store very large N-gram collections.  The
              corresponding directory structure  can  be  created
              using  the  script  mmaakkee--ggooooggllee--nnggrraammss described in
              ttrraaiinniinngg--ssccrriippttss(1).

       --iinntteerrsseecctt _f_i_l_e
              Limit reading of counts via --rreeaadd and  --rreeaadd--ggooooggllee
              to  a  subsect  of N-grams defined by another count
              _f_i_l_e.  (The counts in _f_i_l_e are ignored, only the N-
              grams are relevant.)

       --wwrriittee _f_i_l_e
              Write total counts to _f_i_l_e.

       --wwrriittee--bbiinnaarryy _f_i_l_e
              Write  total  counts  to  _f_i_l_e  in  binary  format.
              Binary count files cannot  be  compressed  and  are
              typically larger than compressed ascii count files.
              However, they can be loaded faster, especially when
              the --lliimmiitt--vvooccaabb option is used.

       --wwrriittee--oorrddeerr _n
              Order  of counts to write.  The default is 0, which
              stands for N-grams of all lengths.

       --wwrriittee_n _f_i_l_e
              where _n is 1, 2, 3, 4, 5, 6, 7, 8,  or  9.   Writes
              only  counts  of the indicated order to _f_i_l_e.  This
              is  convenient  to  generate  counts  of  different
              orders separately in a single pass.

       --ssoorrtt  Output  counts  in lexicographic order, as required
              for nnggrraamm--mmeerrggee(1).

       --rreeccoommppuuttee
              Regenerate lower-order counts by summing the  high-
              est-order counts for each N-gram prefix.

       --lliimmiitt--vvooccaabb
              Discard  N-gram  counts on reading that do not per-
              tain to the words specified in the vocabulary.  The
              default  is  that words used in the count files are
              automatically added to the vocabulary.

   LLMM OOppttiioonnss
       --llmm _l_m_f_i_l_e
              Estimate a backoff  N-gram  model  from  the  total
              counts,  and write it to _l_m_f_i_l_e in nnggrraamm--ffoorrmmaatt(5).

       --wwrriittee--bbiinnaarryy--llmm
              Output _l_m_f_i_l_e in binary format.   If  an  LM  class
              does not provide a binary format the default (text)
              format will be output instead.

       --nnoonneevveennttss _f_i_l_e
              Read a list of words from _f_i_l_e that are to be  con-
              sidered  non-events,  i.e.,  that can only occur in
              the context of an N-gram.   Such  words  are  given
              zero probability mass in model estimation.

       --ffllooaatt--ccoouunnttss
              Enable  manipulation  of  fractional  counts.  Only
              certain  discounting  methods  support  non-integer
              counts.

       --sskkiipp  Estimate  a ``skip'' N-gram model, which predicts a
              word by an interpolation of the  immediate  context
              and the context one word prior.  This also triggers
              N-gram counts to be generated  that  are  one  word
              longer  than  the  indicated  order.  The following
              four options control the  EM  estimation  algorithm
              used for skip-N-grams.

       --iinniitt--llmm _l_m_f_i_l_e
              Load  an  LM  to  initialize  the parameters of the
              skip-N-gram.

       --sskkiipp--iinniitt _v_a_l_u_e
              The initial skip probability for all words.

       --eemm--iitteerrss _n
              The maximum number of EM iterations.

       --eemm--ddeellttaa _d
              The convergence criterion for EM: if  the  relative
              change  in  log  likelihood  falls  below the given
              value, iteration stops.

       --ccoouunntt--llmm
              Estimate  a  count-based  interpolated   LM   using
              Jelinek-Mercer  smoothing  (Chen  & Goodman, 1998).
              Several of the options for skip-N-gram LMs  (above)
              apply.  An initial count-LM in the format described
              in nnggrraamm(1) needs to be specified  using  --iinniitt--llmm.
              The  options --eemm--iitteerrss and --eemm--ddeellttaa control termi-
              nation of the EM algorithm.  Note that  the  N-gram
              counts  used  to  estimate  the  maximum-likelihood
              estimates come from the --iinniitt--llmm model.  The counts
              specified  with  --rreeaadd  or  --tteexxtt  are used only to
              estimate the smoothing (interpolation weights).

       --uunnkk   Build an ``open vocabulary''  LM,  i.e.,  one  that
              contains  the unknown-word token as a regular word.
              The default is to remove the unknown word.

       --mmaapp--uunnkk _w_o_r_d
              Map out-of-vocabulary words to  _w_o_r_d,  rather  than
              the default <<uunnkk>> tag.

       --ttrruusstt--ttoottaallss
              Force  the  lower-order  counts to be used as total
              counts in estimating N-gram probabilities.  Usually
              these  totals  are recomputed from the higher-order
              counts.

       --pprruunnee _t_h_r_e_s_h_o_l_d
              Prune N-gram probabilities if their removal  causes
              (training  set) perplexity of the model to increase
              by less than _t_h_r_e_s_h_o_l_d relative.

       --mmiinnpprruunnee _n
              Only prune N-grams  of  length  at  least  _n.   The
              default  (and  minimum  allowed  value) is 2, i.e.,
              only unigrams are excluded from pruning.

       --ddeebbuugg _l_e_v_e_l
              Set debugging output from estimated  LM  at  _l_e_v_e_l.
              Level 0 means no debugging.  Debugging messages are
              written to stderr.

       --ggtt_nmmiinn _c_o_u_n_t
              where _n is 1, 2, 3, 4, 5, 6, 7, 8, or 9.   Set  the
              minimal  count  of  N-grams of order _n that will be
              included in the LM.   All  N-grams  with  frequency
              lower  than  that will effectively be discounted to
              0.  If _n is omitted the parameter  for  N-grams  of
              order > 9 is set.
              NOTE:  This  option  affects  not  only the default
              Good-Turing discounting but  the  alternative  dis-
              counting methods described below as well.

       --ggtt_nmmaaxx _c_o_u_n_t
              where  _n  is 1, 2, 3, 4, 5, 6, 7, 8, or 9.  Set the
              maximal count of N-grams of order _n that  are  dis-
              counted  under  Good-Turing.  All N-grams more fre-
              quent than that  will  receive  maximum  likelihood
              estimates.  Discounting can be effectively disabled
              by setting this to 0.  If _n is omitted the  parame-
              ter for N-grams of order > 9 is set.

       In  the following discounting parameter options, the order
       _n may be omitted, in which case a default for  all  N-gram
       orders  is set.  The corresponding discounting method then
       becomes the default method for all orders, unless specifi-
       cally  overridden  by an option with _n.  If no discounting
       method is specified, Good-Turing is used.

       --ggtt_n _g_t_f_i_l_e
              where _n is 1, 2, 3, 4, 5, 6, 7, 8, or 9.   Save  or
              retrieve  Good-Turing  parameters (cutoffs and dis-
              counting factors) in/from _g_t_f_i_l_e.  This  is  useful
              as  GT  parameters should always be determined from
              unlimited vocabulary counts, whereas  the  eventual
              LM  may  use  a  limited vocabulary.  The parameter
              files may also be hand-edited.  If an --llmm option is
              specified  the  GT parameters are read from _g_t_f_i_l_e,
              otherwise they are computed from the current counts
              and saved in _g_t_f_i_l_e.

       --ccddiissccoouunntt_n _d_i_s_c_o_u_n_t
              where _n is 1, 2, 3, 4, 5, 6, 7, 8, or 9.  Use Ney's
              absolute discounting for N-grams of order _n,  using
              _d_i_s_c_o_u_n_t as the constant to subtract.

       --wwbbddiissccoouunntt_n
              where  _n is 1, 2, 3, 4, 5, 6, 7, 8, or 9.  Use Wit-
              ten-Bell discounting for N-grams of order _n.  (This
              is the estimator where the first occurrence of each
              word is taken to be a  sample  for  the  ``unseen''
              event.)

       --nnddiissccoouunntt_n
              where  _n is 1, 2, 3, 4, 5, 6, 7, 8, or 9.  Use Ris-
              tad's natural discounting law for N-grams of  order
              _n.

       --aaddddssmmooootthh_n _d_e_l_t_a
              where _n is 1, 2, 3, 4, 5, 6, 7, 8, or 9.  Smooth by
              adding _d_e_l_t_a to each N-gram count.  This is usually
              a  poor  smoothing  method  and included mainly for
              instructional purposes.

       --kknnddiissccoouunntt_n
              where _n is 1, 2, 3, 4, 5, 6, 7, 8, or 9.  Use  Chen
              and  Goodman's  modified Kneser-Ney discounting for
              N-grams of order _n.

       --kknn--ccoouunnttss--mmooddiiffiieedd
              Indicates that input counts have already been modi-
              fied  for  Kneser-Ney smoothing.  If this option is
              not  given,  the  KN  discounting  method  modifies
              counts  (except those of highest order) in order to
              estimate the backoff distributions.  When using the
              --wwrriittee  and related options the output will reflect
              the modified counts.

       --kknn--mmooddiiffyy--ccoouunnttss--aatt--eenndd
              Modify Kneser-Ney counts after estimating discount-
              ing   constants,  rather  than  before  as  is  the
              default.

       --kknn_n _k_n_f_i_l_e
              where _n is 1, 2, 3, 4, 5, 6, 7, 8, or 9.   Save  or
              retrieve  Kneser-Ney  parameters  (cutoff  and dis-
              counting constants) in/from _k_n_f_i_l_e.  This is useful
              as smoothing parameters should always be determined
              from unlimited vocabulary counts, whereas the even-
              tual  LM may use a limited vocabulary.  The parame-
              ter files may  also  be  hand-edited.   If  an  --llmm
              option is specified the KN parameters are read from
              _k_n_f_i_l_e, otherwise they are computed from  the  cur-
              rent counts and saved in _k_n_f_i_l_e.

       --uukknnddiissccoouunntt_n
              where  _n  is 1, 2, 3, 4, 5, 6, 7, 8, or 9.  Use the
              original (unmodified) Kneser-Ney discounting method
              for N-grams of order _n.

       --iinntteerrppoollaattee_n
              where  _n  is  1, 2, 3, 4, 5, 6, 7, 8, or 9.  Causes
              the discounted N-gram probability estimates at  the
              specified  order  _n  to be interpolated with lower-
              order estimates.  (The result of the  interpolation
              is  encoded  as a standard backoff model and can be
              evaluated as such -- the interpolation  happens  at
              estimation  time.)   This  sometimes  yields better
              models with some  smoothing  methods  (see  Chen  &
              Goodman,  1998).   Only  Witten-Bell, absolute dis-
              counting, and  (original  or  modified)  Kneser-Ney
              smoothing currently support interpolation.

       --mmeettaa--ttaagg _s_t_r_i_n_g
              Interpret  words  starting with _s_t_r_i_n_g as count-of-
              count (meta-count) tags.  For example, an N-gram
                   a b _s_t_r_i_n_g3    4
              means that there were 4 trigrams starting  with  "a
              b"  that occurred 3 times each.  Meta-tags are only
              allowed in the last position of an N-gram.
              Note: when using --ttoolloowweerr the meta-tag _s_t_r_i_n_g  must
              not contain any uppercase characters.

       --rreeaadd--wwiitthh--mmiinnccoouunnttss
              Save memory by eliminating N-grams with counts that
              fall below the thresholds set  by  --ggtt_Nmmiinn  options
              during  --rreeaadd  operation  (this  assumes  the input
              counts contain no  duplicate  N-grams).   Also,  if
              --mmeettaa--ttaagg  is defined, these low-count N-grams will
              be converted to  count-of-count  N-grams,  so  that
              smoothing  methods that need this information still
              work correctly.

SSEEEE AALLSSOO
       ngram-merge(1),   ngram(1),   ngram-class(1),    training-
       scripts(1), lm-scripts(1), ngram-format(5).
       S. F. Chen and J. Goodman, ``An Empirical Study of Smooth-
       ing Techniques for Language Modeling,'' TR-10-98, Computer
       Science Group, Harvard Univ., 1998.
       S. M. Katz, ``Estimation of Probabilities from Sparse Data
       for the Language Model Component of a Speech Recognizer,''
       _I_E_E_E _T_r_a_n_s_. _A_S_S_P 35(3), 400-401, 1987.
       R.  Kneser  and  H. Ney, ``Improved backing-off for M-gram
       language modeling,'' _P_r_o_c_. _I_C_A_S_S_P, 181-184, 1995.
       H. Ney and U. Essen, ``On Smoothing Techniques for Bigram-
       based Natural Language Modelling,'' _P_r_o_c_. _I_C_A_S_S_P, 825-828,
       1991.
       E.  S.  Ristad,  ``A  Natural  Law  of  Succession,''  CS-
       TR-495-95, Comp. Sci. Dept., Princeton Univ., 1995.
       I. H. Witten and T. C. Bell, ``The Zero-Frequency Problem:
       Estimating the Probabilities of Novel Events  in  Adaptive
       Text  Compression,'' _I_E_E_E _T_r_a_n_s_. _I_n_f_o_r_m_a_t_i_o_n _T_h_e_o_r_y 37(4),
       1085-1094, 1991.

BBUUGGSS
       Several of the LM types supported by nnggrraamm(1)  don't  have
       explicit  support in nnggrraamm--ccoouunntt.  Instead, they are built
       by separately  manipulating  N-gram  counts,  followed  by
       standard N-gram model estimation.
       LM support for tagged words is incomplete.
       Only  absolute  and Witten-Bell discounting currently sup-
       port fractional counts.
       The combination of --rreeaadd--wwiitthh--mmiinnccoouunnttss and --mmeettaa--ttaagg pre-
       serves enough count-of-count information for _a_p_p_l_y_i_n_g dis-
       counting parameters to the input counts, but it  does  not
       necessarily  allow  the  parameters  to be correctly _e_s_t_i_-
       _m_a_t_e_d.  Therefore, discounting parameters should always be
       estimated  from full counts (e.g., using the helper ttrraaiinn--
       iinngg--ssccrriippttss(1)), and then read from files.

AAUUTTHHOORR
       Andreas Stolcke <stolcke@speech.sri.com>.
       Copyright 1995-2008 SRI International



