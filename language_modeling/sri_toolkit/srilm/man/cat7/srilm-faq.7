SRILM-FAQ(1)                                         SRILM-FAQ(1)



NNAAMMEE
       SRILM-FAQ - Frequently asked questions about SRI LM tools

SSYYNNOOPPSSIISS
       man srilm-faq

DDEESSCCRRIIPPTTIIOONN
       This  document tries to answer some of the most frequently
       asked questions about SRILM.

   BBuuiilldd iissssuueess
       AA11)) II rraann ````mmaakkee WWoorrlldd'''' bbuutt tthhee  $$SSRRIILLMM//bbiinn//$$MMAACCHHIINNEE__TTYYPPEE
       ddiirreeccttoorryy iiss eemmppttyy..
           Building the binaries can fail for a variety  of  rea-
           sons.  Check the following:

           a)     Make  sure  the  SRILM  environment variable is
                  set, or specified on  the  make  command  line,
                  e.g.:
                       make SRILM=$PWD

           b)     Make  sure  the $$SSRRIILLMM//ssbbiinn//mmaacchhiinnee--ttyyppee script
                  returns a valid string for the platform you are
                  trying  to  build  on.   Known  platforms  have
                  machine-specific makefiles called
                       $SRILM/common/Makefile.machine.$MACHINE_TYPE
                  If mmaacchhiinnee--ttyyppee does not work for some  reason,
                  you  can  override  its  output  on the command
                  line:
                       make MACHINE_TYPE=xyz
                  If you are building for an unsupported platform
                  create a new machine-specific makefile and mail
                  it to stolcke@speech.sri.com.

           c)     Make sure your compiler works  and  is  invoked
                  correctly.   You will probably have to edit the
                  CCCC and CCXXXX variables in  the  platform-specific
                  makefile.  If you have questions about compiler
                  invocation and best  options  consult  a  local
                  expert;  these  things  differ  widely  between
                  sites.

           d)     The default is to  compile  with  Tcl  support.
                  This  is  in  fact  only  used for some testing
                  binaries (which are not built by  default),  so
                  it can be turned off if Tcl is not available or
                  presents problems.  Edit  the  machine-specific
                  makefile  accordingly.   To use Tcl, locate the
                  ttccll..hh header file and the library  itself,  and
                  set (for example)
                       TCL_INCLUDE = -I/path/to/include
                       TCL_LIBRARY = -L/path/to/lib -ltcl8.4
                  To disable Tcl support set
                       NO_TCL = X
                       TCL_INCLUDE =
                       TCL_LIBRARY =

       AA22)) TThhee rreeggrreessssiioonn tteesstt oouuttppuuttss ddiiffffeerr ffoorr aallll tteessttss.. WWhhaatt
       ddiidd II ddoo wwrroonngg??
           Most  likely  the  binaries didn't get built or aren't
           executable for some reason.  Check issue A1).

       AA33)) II ggeett ddiiffffeerriinngg oouuttppuuttss ffoorr  ssoommee  ooff  tthhee  rreeggrreessssiioonn
       tteessttss.. IIss tthhaatt OOKK??
           It might be.  The comparison of  reference  to  actual
           output  allows  for  small  numerical differences, but
           some of the algorithms make hard  decisions  based  on
           floating-point computations that can result in differ-
           ent outputs as a result of  different  compiler  opti-
           mizations,  machine  floating  point precisions (Intel
           versus IEEE format), and math libraries.  Test of this
           nature include nnggrraamm--ccllaassss, ddiissaammbbiigg, and nnbbeesstt--rroovveerr.
           When encountering differences, diff the output in  the
           $SRILM/test/outputs/_T_E_S_T.$MACHINE_TYPE.stdout  file to
           the  corresponding  $SRILM/test/reference/_T_E_S_T.stdout,
           where  _T_E_S_T is the name of the test that failed.  Also
           compare the corresponding .stderr  files;  differences
           there  usually indicate operating-system related prob-
           lems.

   LLaarrggee ddaattaa aanndd mmeemmoorryy iissssuueess
       BB11)) II''mm ggeettttiinngg aa mmeessssaaggee ssaayyiinngg ````AAsssseerrttiioonn ``bbooddyy  !!==  00''
       ffaaiilleedd..''''
           You are running out of memory.  See  subsequent  ques-
           tions depending on what you are trying to do.

       Note:
           The  above  message means you are running out of "vir-
           tual" memory on your computer, which could be  because
           of  limits in swap space, administrative resource lim-
           its, or limitations of  the  machine  architecture  (a
           32-bit  machine cannot address more than 4GB no matter
           how many resources your system has).  Another  symptom
           of  not  enough  memory is that your program runs, but
           very, very slowly, i.e., it is "paging" or  "swapping"
           as  it  tries  to use more memory than the machine has
           RAM installed.

       BB22)) II aamm ttrryyiinngg ttoo ccoouunntt NN--ggrraammss iinn aa tteexxtt ffiillee  aanndd  rruunn--
       nniinngg oouutt ooff mmeemmoorryy..
           Don't  use  nnggrraamm--ccoouunntt  directly  to  count  N-grams.
           Instead,  use  the  mmaakkee--bbaattcchh--ccoouunnttss and mmeerrggee--bbaattcchh--
           ccoouunnttss scripts described in ttrraaiinniinngg--ssccrriippttss(1).  That
           way  you  can create N-gram counts limited only by the
           maximum file size on your system.

       BB33)) II aamm ttrryyiinngg ttoo bbuuiilldd aann NN--ggrraamm LLMM aanndd nnggrraamm--ccoouunntt rruunnss
       oouutt ooff mmeemmoorryy..
           You are running out of memory either  because  of  the
           size  of  ngram  counts, or of the LM being built. The
           following  are  strategies  for  reducing  the  memory
           requirements for training LMs.

           a)     Assuming  you  are using Good-Turing or Kneser-
                  Ney discounting, don't use nnggrraamm--ccoouunntt in "raw"
                  form.   Instead,  use  the  mmaakkee--bbiigg--llmm wrapper
                  script described in the ttrraaiinniinngg--ssccrriippttss(1) man
                  page.

           b)     Switch  to  using  the "_c" or "_s" versions of
                  the SRI binaries.  For instructions on  how  to
                  build  them, see the INSTALL file.  Once built,
                  set your executable  search  path  accordingly,
                  and try mmaakkee--bbiigg--llmm again.

           c)     Lower  the  minimum counts for N-grams included
                  in the LM, i.e.,  the  values  of  the  options
                  --ggtt22mmiinn,  --ggtt33mmiinn,  --ggtt44mmiinn,  etc.   The higher
                  order  N-grams  typically  get  higher  minimum
                  counts.

           d)     Get  a  machine  with  more memory.  If you are
                  hitting the limitations  of  a  32-bit  machine
                  architecture,  get  a 64-bit machine and recom-
                  pile SRILM to take advantage  of  the  expanded
                  address space.  (The MACHINE_TYPE=i686-m64 set-
                  ting is for systems based on 64-bit AMD proces-
                  sors,   as  well  as  recent  compatibles  from
                  Intel.)  Note that 64-bit pointers will require
                  a  memory  overhead  in themselves, so you will
                  need a machine with significantly, not  just  a
                  little, more memory than 4GB.

       BB44))  II  aamm  ttrryyiinngg ttoo aappppllyy aa llaarrggee LLMM ttoo ssoommee ddaattaa aanndd aamm
       rruunnnniinngg oouutt ooff mmeemmoorryy..
           Again,  there  are several strategies to reduce memory
           requirements.

           a)     Use the "_c" or "_s" versions of the SRI  bina-
                  ries.  See 3b) above.

           b)     Precompute the vocabulary of your test data and
                  use the nnggrraamm --lliimmiitt--vvooccaabb option to load  only
                  the  N-gram  parameters  relevant to your data.
                  This approach should allow you to use arbitrar-
                  ily large LMs provided the data is divided into
                  small enough chunks.

           c)     If the LM can be built on a large machine,  but
                  then  is  to  be  used on machines with limited
                  memory, use nnggrraamm --pprruunnee  to  remove  the  less
                  important  parameters  of the model.  This usu-
                  ally gives huge size reductions with relatively
                  modest  performance  degradation.  The tradeoff
                  is adjustable by varying the pruning parameter.

       BB55))  HHooww  ccaann II rreedduuccee tthhee ttiimmee iitt ttaakkeess ttoo llooaadd llaarrggee LLMMss
       iinnttoo mmeemmoorryy??
           The  techniques  described  in  4b) and 4c) above also
           reduce the load time of the LM.  Additional  steps  to
           try are:

           a)     Convert the LM into binary format, using
                            ngram -order _N -lm _O_L_D_L_M -write-bin-lm _N_E_W_L_M
                  (This  is  currently only supported for N-gram-
                  based LMs.)   You  can  also  generate  the  LM
                  directly in binary format, using
                            ngram-count ... -lm _N_E_W_L_M -write-binary-lm
                  The  resulting  _N_E_W_L_M file (which should not be
                  compressed) can be used in place of  a  textual
                  LM  file with all compiled SRILM tools (but not
                  with llmm--ssccrriippttss(1)).  The  format  is  machine-
                  independent,  i.e.,  it can be read on machines
                  with different word sizes or byte-order.  Load-
                  ing binary LMs is faster because (1) it reduces
                  the overhead of parsing the input data, and (2)
                  in combination with --lliimmiitt--vvooccaabb (see 4b) it is
                  much faster to skip sections of the LM that are
                  out-of-vocabulary.

           Note:  There  is  also  a  binary  format  for  N-gram
                  counts.  It can be generated using
                            ngram-count -write-binary _C_O_U_N_T_S
                  and has similar advantages as binary LM  files.

           b)     Start  a "probability server" that loads the LM
                  ahead of time, and then have "LM clients" query
                  the  server instead of computing the probabili-
                  ties themselves.
                  The server is started on a machine  named  _H_O_S_T
                  using
                            ngram _L_M_O_P_T_I_O_N_S -server-port _P &
                  where _P is an integer < 2^16 that specifies the
                  TCP/IP port number the server will  listen  on,
                  and _L_M_O_P_T_I_O_N_S are whatever options necessary to
                  define the LM to be used.
                  One or more clients (programs such as nnggrraamm(1),
                  ddiissaammbbiigg(1),  llaattttiiccee--ttooooll(1))  can  then query
                  the server using the options
                            -use-server _P@_H_O_S_T -cache-served-ngrams
                  instead of the usual "-lm _F_I_L_E".   The  --ccaacchhee--
                  sseerrvveedd--nnggrraammss  option is not required but often
                  speeds up performance  dramatically  by  saving
                  the results of server lookups in the client for
                  reuse.  Server-based LMs may be  combined  with
                  file-based  LMs  by interpolation; see nnggrraamm(1)
                  for details.

       BB66)) HHooww ccaann II uussee tthhee GGooooggllee WWeebb NN--ggrraamm ccoorrppuuss ttoo bbuuiilldd aann
       LLMM??
           Google has made a corpus of 5-grams extracted  from  1
           tera-words  of  web  data available via LDC.  However,
           the data is too large to build a standard  backoff  N-
           gram,  even  using  the  techniques  described  above.
           Instead, we recommend a "count-based" LM smoothed with
           deleted interpolation.  Such an LM computes probabili-
           ties on the fly from the counts,  of  which  only  the
           subsets  needed for a given test set need to be loaded
           into memory.  LM construction proceeds in the  follow-
           ing steps:

           a)     Make  sure  you  have built SRI binaries either
                  for      a      64-bit      machine      (e.g.,
                  MACHINE_TYPE=i686-m64   OPTION=_c)   or   using
                  64-bit counts (OPTION=_l).  This  is  necessary
                  because the data contains N-gram counts exceed-
                  ing the range of 32-bit integers.  Be  sure  to
                  invoke all commands below using the path to the
                  appropriate binary executable directory.

           b)     Prepare mapping file for some  vocabulary  mis-
                  matches and call this ggooooggllee..aalliiaasseess:
                       <S> <s>
                       </S> </s>
                       <UNK> <unk>

           c)     Prepare  an  initial  count-LM  parameter  file
                  ggooooggllee..ccoouunnttllmm..00:
                       order 5
                       vocabsize 13588391
                       totalcount 1024908267229
                       countmodulus 40
                       mixweights 15
                        0.5 0.5 0.5 0.5 0.5
                        0.5 0.5 0.5 0.5 0.5
                        0.5 0.5 0.5 0.5 0.5
                        0.5 0.5 0.5 0.5 0.5
                        0.5 0.5 0.5 0.5 0.5
                        0.5 0.5 0.5 0.5 0.5
                        0.5 0.5 0.5 0.5 0.5
                        0.5 0.5 0.5 0.5 0.5
                        0.5 0.5 0.5 0.5 0.5
                        0.5 0.5 0.5 0.5 0.5
                        0.5 0.5 0.5 0.5 0.5
                        0.5 0.5 0.5 0.5 0.5
                        0.5 0.5 0.5 0.5 0.5
                        0.5 0.5 0.5 0.5 0.5
                        0.5 0.5 0.5 0.5 0.5
                        0.5 0.5 0.5 0.5 0.5
                       google-counts _P_A_T_H
                  where _P_A_T_H points to the location of the Google
                  N-grams,  i.e., the directory containing subdi-
                  rectories "1gms", "2gms", etc.  Note  that  the
                  vvooccaabbssiizzee and ttoottaallccoouunntt were obtained from the
                  1gms/vocab.gz  and  1gms/total  files,  respec-
                  tively.   (Check  that they match and modify as
                  needed.)  For an explanation of the  parameters
                  see the nnggrraamm(1) --ccoouunntt--llmm option.

           d)     Prepare  a  text file ttuunnee..tteexxtt containing data
                  for estimating the mixture weights.  This  data
                  should be representative of, but different from
                  your test data.  Compute the vocabulary of this
                  data using
                       ngram-count -text tune.text -write-vocab tune.vocab
                  The  vocabulary  size  should  not exceed a few
                  thousand to keep  memory  requirements  in  the
                  following steps manageable.

           e)     Estimate the mixture weights:
                       ngram-count -debug 1 -order 5 -count-lm  \
                            -text tune.text -vocab tune.vocab \
                            -vocab-aliases google.aliases \
                            -limit-vocab \
                            -init-lm google.countlm.0 \
                            -em-iters 100 \
                            -lm google.countlm
                  This   will   write   the   estimated   LM   to
                  ggooooggllee..ccoouunnttllmm.  The output will  be  identical
                  to  the initial LM file, except for the updated
                  interpolation weights.

           f)     Prepare a test data  file  tteesstt..tteexxtt,  and  its
                  vocabulary  tteesstt..vvooccaabb  as  in  Step  d) above.
                  Then apply the LM to the test data:
                       ngram -debug 2 -order 5 -count-lm \
                            -lm google.countlm \
                            -vocab test.vocab \
                            -vocab-aliases google.aliases \
                            -limit-vocab \
                            -ppl test.text > test.ppl
                  The perplexity output will appear in  tteesstt..ppppll..

           g)     Note  that  the  Google  data  uses  mixed case
                  spellings.  To apply the LM to  lowercase  data
                  one  needs  to  prepare  a  much more extensive
                  vocabulary mapping table for the --vvooccaabb--aalliiaasseess
                  option,  namely,  one  that maps all upper- and
                  mixed-case  spellings  to  lowercase   strings.
                  This  mapping  file should be restricted to the
                  words appearing  in  ttuunnee..tteexxtt  and  tteesstt..tteexxtt,
                  respectively,  to avoid defeating the effect of
                  --lliimmiitt--vvooccaabb ..

   SSmmooootthhiinngg iissssuueess
       CC11)) WWhhaatt iiss ssmmooootthhiinngg aanndd ddiissccoouunnttiinngg aallll aabboouutt??
           _S_m_o_o_t_h_i_n_g refers to methods that assign  probabilities
           to  events (N-grams) that do not occur in the training
           data.  According to a pure maximum-likelihood  estima-
           tor these events would have probability zero, which is
           plainly wrong since previously unseen events  in  gen-
           eral  do  occur in independent test data.  Because the
           probability mass is redistributed away from  the  seen
           events toward the unseen events the resulting model is
           "smoother" (closer to  uniform)  than  the  ML  model.
           _D_i_s_c_o_u_n_t_i_n_g  refers  to  the  approach  used  by  many
           smoothing methods of adjusting the empirical counts of
           seen   events  downwards.   The  ML  estimator  (count
           divided by total number of events) is then applied  to
           the  discounted  count,  resulting in a smoother esti-
           mate.

       CC22)) WWhhaatt ssmmooootthhiinngg mmeetthhooddss aarree tthheerree??
           There are many, and SRILM implements are fairly  large
           selection  of  the most popular ones.  A detailed dis-
           cussion of these is  found  in  a  separate  document,
           nnggrraamm--ddiissccoouunntt(7).

       CC33)) WWhhyy aamm II ggeettttiinngg eerrrroorrss oorr wwaarrnniinnggss ffrroomm tthhee ssmmooootthhiinngg
       mmeetthhoodd II''mm uussiinngg??
           The  Good-Turing and Kneser-Ney smoothing methods rely
           on statistics called "count-of-counts", the number  of
           words  occurring  one,  twice,  three times, etc.  The
           formulae for these methods  become  undefined  if  the
           counts-of-counts are zero, or not strictly decreasing.
           Some conditions are fatal (such as when the  count  of
           singleton  words is zero), others lead to less smooth-
           ing (and warnings).  To avoid  these  problems,  check
           for the following possibilities:

           a)     The data could be very sparse, i.e., the train-
                  ing corpus very small.  Try using  the  Witten-
                  Bell discounting method.

           b)     The  vocabulary  could  be  very small, such as
                  when training an  LM  based  on  characters  or
                  parts-of-speech.  Smoothing is less of an issue
                  in those  cases,  and  the  Witten-Bell  method
                  should work well.

           c)     The  data was manipulated in some way, or arti-
                  ficially generated.  For  example,  duplicating
                  data  eliminates  the  odd-numbered  counts-of-
                  counts.

           d)     The vocabulary is limited during counts collec-
                  tion  using the nnggrraamm--ccoouunntt --vvooccaabb option, with
                  the effect that many low-frequency N-grams  are
                  eliminated.   The proper approach is to compute
                  smoothing parameters on  the  full  vocabulary.
                  This  happens  automatically in the mmaakkee--bbiigg--llmm
                  wrapper script, which is preferable  to  direct
                  use of nnggrraamm--ccoouunntt for other reasons (see issue
                  B3-a above).

           e)     You are estimating an  LM  from  N-gram  counts
                  that  have  been truncated beforehand, e.g., by
                  removing singleton events.  If  you  cannot  go
                  back  to  the  original  data and recompute the
                  counts there is a heuristic to extrapolate  low
                  counts-of-counts from higher ones.  The heuris-
                  tic   is   invoked   automatically   (and    an
                  informational message is output) when mmaakkee--bbiigg--
                  llmm is used  to  estimate  LMs  with  Kneser-Ney
                  smoothing.   For  details  see  the paper by W.
                  Wang et al. in  ASRU-2007,  listed  under  "SEE
                  ALSO".

   OOuutt ooff vvooccaabbuullaarryy wwoorrddss
       DD11))  WWhhaatt  iiss tthhee ppeerrpplleexxiittyy ooff aann OOOOVV ((oouutt ooff vvooccaabbuullaarryy))
       wwoorrdd??
           By  default any word not observed in the training data
           is considered OOV and OOV words are  silently  ignored
           by  the  nnggrraamm(1) during perplexity (ppl) calculation.
           For example:

                $ ngram-count -text turkish.train -lm turkish.lm
                $ ngram -lm turkish.lm -ppl turkish.test
                file turkish.test: 61031 sentences, 1000015 words, 34153 OOVs
                0 zeroprobs, logprob= -3.20177e+06 ppl= 1311.97 ppl1= 2065.09

           The statistics printed in the last two lines have  the
           following meanings:

           3344115533 OOOOVVss
                  This is the number of unknown word tokens, i.e.
                  tokens that appear in ttuurrkkiisshh..tteesstt but  not  in
                  ttuurrkkiisshh..ttrraaiinn  from which ttuurrkkiisshh..llmm was gener-
                  ated.

           llooggpprroobb== --33..2200117777ee++0066
                  This gives us the total  logprob  ignoring  the
                  34153  unknown  word  tokens.  The logprob does
                  include  the  probabilities  assigned  to  </s>
                  tokens  which are introduced by nnggrraamm--ccoouunntt(1).
                  Thus the total number of tokens which this log-
                  prob is based on is
                       words - OOVs + sentences = 1000015 - 34153 + 61031

           ppppll == 11331111..9977
                  This gives us the geometric average of 1/proba-
                  bility of each token,  i.e.,  perplexity.   The
                  exact expression is:
                       ppl = 10^(-logprob / (words - OOVs + sentences))

           ppppll11 == 22006655..0099
                  This  gives  us the average perplexity per word
                  excluding the </s> tokens.  The  exact  expres-
                  sion is:
                       ppl1 = 10^(-logprob / (words - OOVs))
       You  can verify these numbers by running the nnggrraamm program
       with the --ddeebbuugg 22  option,  which  gives  the  probability
       assigned to each token.

       DD22)) WWhhaatt hhaappppeennss wwhheenn tthhee OOOOVV wwoorrdd iiss iinn tthhee ccoonntteexxtt ooff aann
       NN--ggrraamm??
           Exact  details  depend  on  the  discounting algorithm
           used, but typically the backed-off probability from  a
           lower  order  N-gram  is  used.  If the --uunnkk option is
           used as explained below, an <unk> token is assumed  to
           take  the place of the OOV word and no back-off may be
           necessary if a corresponding N-gram  containing  <unk>
           is found in the LM.

       DD33)) IIssnn''tt iitt wwrroonngg ttoo aassssiiggnn 00 llooggpprroobb ttoo OOOOVV wwoorrddss??
           That depends on the application.  If you are comparing
           multiple language models which all consider  the  same
           set  of words as OOV it may be OK to ignore OOV words.
           Note that perplexity comparisons are only  ever  mean-
           ingful  if  the  vocabularies of all LMs are the same.
           Therefore, to compare LMs with different sets  of  OOV
           words  (such  as  when  using  different  tokenization
           strategies for morphologically complex languages) then
           it  becomes  important  to  take into account the true
           cost of the OOV words, or to model all words,  includ-
           ing OOVs.

       DD44))  HHooww  ddoo  II ttaakkee iinnttoo aaccccoouunntt tthhee ttrruuee ccoosstt ooff tthhee OOOOVV
       wwoorrddss??
           A simple strategy is to "explode" the OOV words, i.e.,
           split them into characters in the  training  and  test
           data.   Typically  words that appear more than once in
           the training data  are  considered  to  be  vocabulary
           words.   All  other words are split into their charac-
           ters and  the  individual  characters  are  considered
           tokens.   Assuming  that all characters occur at least
           once in the training data there will be no OOV  tokens
           in the test data.  Note that this strategy changes the
           number of tokens in the data set, so even though  log-
           prob  is  meaningful  be  careful  when  reporting ppl
           results.

       DD55)) WWhhaatt iiff II wwaanntt ttoo mmooddeell tthhee OOOOVV wwoorrddss eexxpplliicciittllyy??
           Maybe a better strategy is to have a separate "letter"
           model for OOV words.  This can be easily created using
           SRILM by using a training file listing the  OOV  words
           one  per  line  with  their  characters  separated  by
           spaces.   The  nnggrraamm--ccoouunntt  options  --uukknnddiissccoouunntt  and
           --oorrddeerr  77  seem  to  work  well for this purpose.  The
           final logprob  results  are  obtained  in  two  steps.
           First  do  regular  training  and testing on your data
           using --vvooccaabb and --uunnkk options.  The resulting  logprob
           will  include  the cost of the vocabulary words and an
           <unk> token for each OOV word.  Then apply the  letter
           model  to each OOV word in the test set.  Add the log-
           probs.  Here is an example:

                # Determine vocabulary:
                ngram-count -text turkish.train -write-order 1 -write turkish.train.1cnt
                awk '$2>1'  turkish.train.1cnt | cut -f1 | sort > turkish.train.vocab
                awk '$2==1' turkish.train.1cnt | cut -f1 | sort > turkish.train.oov

                # Word model:
                ngram-count -kndiscount -interpolate -order 4 -vocab turkish.train.vocab -unk -text turkish.train -lm turkish.train.model
                ngram -order 4 -unk -lm turkish.train.model -ppl turkish.test > turkish.test.ppl

                # Letter model:
                perl -C -lne 'print join(" ", split(""))' turkish.train.oov > turkish.train.oov.split
                ngram-count -ukndiscount -interpolate -order 7 -text turkish.train.oov.split -lm turkish.train.oov.model
                perl -pe 's/\s+/\n/g' turkish.test | sort > turkish.test.words
                comm -23 turkish.test.words turkish.train.vocab > turkish.test.oov
                perl -C -lne 'print join(" ", split(""))' turkish.test.oov > turkish.test.oov.split
                ngram -order 7 -ppl turkish.test.oov.split -lm turkish.train.oov.model > turkish.test.oov.ppl

                # Add the logprobs in turkish.test.ppl and turkish.test.oov.ppl.

           Again, perplexities are  not  directly  meaningful  as
           computed  by SRILM, but you can recompute them by hand
           using the combined logprob value, and  the  number  of
           original word tokens in the test set.

SSEEEE AALLSSOO
       ngram(1),  ngram-count(1), training-scripts(1), ngram-dis-
       count(7).
       $SRILM/INSTALL
       http://www.speech.sri.com/projects/srilm/mail-
       archive/srilm-user/
       http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?cata-
       logId=LDC2006T13
       W. Wang, A. Stolcke, & J. Zheng, Reranking Machine  Trans-
       lation  Hypotheses  With Structured and Web-based Language
       Models. Proc. IEEE Automatic Speech Recognition and Under-
       standing    Workshop,    pp.    159-164,    Kyoto,   2007.
       http://www.speech.sri.com/cgi-bin/run-dis-
       till?papers/asru2007-mt-lm.ps.gz

BBUUGGSS
       This document is work in progress.

AAUUTTHHOORR
       Andreas Stolcke <stolcke@speech.sri.com>
       Deniz Yuret <dyuret@ku.edu.tr>
       Copyright 2007-2008 SRI International



SRILM Miscellaneous$Date: 2008/01/02 07:13:19 $      SRILM-FAQ(1)
